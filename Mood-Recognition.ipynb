{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib as plt\n",
    "import time, csv, os, shutil, sys\n",
    "\n",
    "\n",
    "\n",
    "j=0\n",
    "label=0\n",
    "trainPath = \"../Mood_Detector/training.csv\"\n",
    "dstPath = \"../Mood_Detector/\"\n",
    "imgPath = \"../Mood_Detector/Manually_Annotated/Manually_Annotated_Images/\"\n",
    "\n",
    "with open(trainPath, 'r') as trainFile:\n",
    "    sp = []\n",
    "    skip_to_line = 0\n",
    "    for lines in trainFile:\n",
    "        skip_to_line += 1\n",
    "        # skip header\n",
    "        if skip_to_line == 1:\n",
    "            for lines in trainFile:\n",
    "                sp = lines.split(',')\n",
    "                # get the label\n",
    "                label = sp[6]\n",
    "                # move the image file path into corresponding target directory\n",
    "                shutil.move(imgPath+sp[0],dstPath + label + \"/\")\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib as plt\n",
    "import time, csv, os, shutil, sys\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "trainPath = \"../Mood_Detector/training.csv\"\n",
    "dstPath = \"../Mood_Detector/\"\n",
    "imgPath = \"../Mood_Detector/Manually_Annotated/Manually_Annotated_Images/\"\n",
    "\n",
    "CATEGORIES = [\"Neutral\",\"Happiness\",\"Sadness\",\"Surprise\",\"Fear\",\"Disgust\",\"Anger\",\"Contempt\"]\n",
    "training_data = []\n",
    "IMG_SIZE = 64\n",
    "i=0\n",
    "\n",
    "\n",
    "def createTrainData():    \n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(dstPath, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))\n",
    "                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
    "                new_array = cv2.resize(img_array,(IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "\n",
    "createTrainData()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for features, labels in training_data:\n",
    "    X.append(features)\n",
    "    y.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = 64\n",
    "\n",
    "X = pickle.load(open(\"X.pickle\",\"rb\"))\n",
    "y = pickle.load(open(\"y.pickle\",\"rb\"))\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63942, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def getHalfData(data):\n",
    "    d1 = data[:int(len(data)/2)]\n",
    "    return d1\n",
    "\n",
    "# i'm Normalizing half the data\n",
    "def dataNormalization(data):\n",
    "    d1 = data[:int(len(data)/2),:,:,:].astype('float32')\n",
    "    d1 /= 255\n",
    "#     d2 = data[int(57547/2):,:,:,:].astype('float16')\n",
    "#     d2 /= 255\n",
    "#     normalized = np.vstack((d1,d2))\n",
    "#     return normalized\n",
    "    return d1\n",
    "    \n",
    "x_train = dataNormalization(x_train)\n",
    "x_test = dataNormalization(x_test)\n",
    "y_train = getHalfData(y_train)\n",
    "y_test = getHalfData(y_test)\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28773, 64, 64, 3)\n",
      "(3197, 64, 64, 3)\n",
      "28773\n",
      "3197\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "# print(y_train)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nadir\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23018 samples, validate on 5755 samples\n",
      "Epoch 1/40\n",
      "23018/23018 [==============================] - 163s 7ms/step - loss: 2.4607 - acc: 0.1537 - val_loss: 2.0282 - val_acc: 0.1491\n",
      "Epoch 2/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 2.2181 - acc: 0.1687 - val_loss: 2.0289 - val_acc: 0.1482\n",
      "Epoch 3/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 2.1460 - acc: 0.1806 - val_loss: 2.0424 - val_acc: 0.1480\n",
      "Epoch 4/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 2.1001 - acc: 0.1901 - val_loss: 2.0255 - val_acc: 0.2083\n",
      "Epoch 5/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 2.0396 - acc: 0.2188 - val_loss: 2.0950 - val_acc: 0.2087\n",
      "Epoch 6/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 1.8758 - acc: 0.3095 - val_loss: 1.8630 - val_acc: 0.3051\n",
      "Epoch 7/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 1.7566 - acc: 0.3503 - val_loss: 1.8111 - val_acc: 0.3500\n",
      "Epoch 8/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 1.6488 - acc: 0.3856 - val_loss: 1.6399 - val_acc: 0.4007\n",
      "Epoch 9/40\n",
      "23018/23018 [==============================] - 154s 7ms/step - loss: 1.5545 - acc: 0.4282 - val_loss: 1.5485 - val_acc: 0.4247\n",
      "Epoch 10/40\n",
      "23018/23018 [==============================] - 153s 7ms/step - loss: 1.4915 - acc: 0.4437 - val_loss: 1.6697 - val_acc: 0.3267\n",
      "Epoch 11/40\n",
      "23018/23018 [==============================] - 153s 7ms/step - loss: 1.4285 - acc: 0.4703 - val_loss: 1.4502 - val_acc: 0.4516\n",
      "Epoch 12/40\n",
      "23018/23018 [==============================] - 153s 7ms/step - loss: 1.3805 - acc: 0.4864 - val_loss: 1.4858 - val_acc: 0.4530\n",
      "Epoch 13/40\n",
      "23018/23018 [==============================] - 153s 7ms/step - loss: 1.3363 - acc: 0.5022 - val_loss: 1.4693 - val_acc: 0.4566\n",
      "Epoch 14/40\n",
      "23018/23018 [==============================] - 156s 7ms/step - loss: 1.2961 - acc: 0.5190 - val_loss: 1.3877 - val_acc: 0.4864\n",
      "Epoch 15/40\n",
      "23018/23018 [==============================] - 156s 7ms/step - loss: 1.2629 - acc: 0.5280 - val_loss: 1.3746 - val_acc: 0.4883\n",
      "Epoch 16/40\n",
      "23018/23018 [==============================] - 156s 7ms/step - loss: 1.2316 - acc: 0.5412 - val_loss: 1.4266 - val_acc: 0.4678\n",
      "Epoch 17/40\n",
      "23018/23018 [==============================] - 156s 7ms/step - loss: 1.2056 - acc: 0.5515 - val_loss: 1.4386 - val_acc: 0.4679\n",
      "Epoch 18/40\n",
      "23018/23018 [==============================] - 153s 7ms/step - loss: 1.1696 - acc: 0.5658 - val_loss: 1.3310 - val_acc: 0.5117\n",
      "Epoch 19/40\n",
      "23018/23018 [==============================] - 154s 7ms/step - loss: 1.1463 - acc: 0.5735 - val_loss: 1.3738 - val_acc: 0.4982\n",
      "Epoch 20/40\n",
      "23018/23018 [==============================] - 154s 7ms/step - loss: 1.1133 - acc: 0.5895 - val_loss: 1.3443 - val_acc: 0.4950\n",
      "Epoch 21/40\n",
      "23018/23018 [==============================] - 153s 7ms/step - loss: 1.0885 - acc: 0.5951 - val_loss: 1.4051 - val_acc: 0.4693\n",
      "Epoch 22/40\n",
      "23018/23018 [==============================] - 153s 7ms/step - loss: 1.0632 - acc: 0.6050 - val_loss: 1.3162 - val_acc: 0.5089\n",
      "Epoch 23/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 1.0340 - acc: 0.6176 - val_loss: 1.3936 - val_acc: 0.4775\n",
      "Epoch 24/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 1.0126 - acc: 0.6228 - val_loss: 1.3016 - val_acc: 0.5107\n",
      "Epoch 25/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.9780 - acc: 0.6378 - val_loss: 1.3009 - val_acc: 0.5171\n",
      "Epoch 26/40\n",
      "23018/23018 [==============================] - 154s 7ms/step - loss: 0.9534 - acc: 0.6445 - val_loss: 1.3435 - val_acc: 0.5022\n",
      "Epoch 27/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.9195 - acc: 0.6573 - val_loss: 1.3613 - val_acc: 0.4987\n",
      "Epoch 28/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.8949 - acc: 0.6677 - val_loss: 1.3760 - val_acc: 0.5018\n",
      "Epoch 29/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.8667 - acc: 0.6772 - val_loss: 1.3588 - val_acc: 0.5096\n",
      "Epoch 30/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.8351 - acc: 0.6897 - val_loss: 1.8006 - val_acc: 0.4078\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.2.\n",
      "Epoch 31/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.6595 - acc: 0.7558 - val_loss: 1.3531 - val_acc: 0.5220\n",
      "Epoch 32/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.6048 - acc: 0.7726 - val_loss: 1.3906 - val_acc: 0.5258\n",
      "Epoch 33/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.5649 - acc: 0.7894 - val_loss: 1.3952 - val_acc: 0.5239\n",
      "Epoch 34/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.5425 - acc: 0.7999 - val_loss: 1.4424 - val_acc: 0.5291\n",
      "Epoch 35/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.5207 - acc: 0.8050 - val_loss: 1.4555 - val_acc: 0.5307\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.04000000059604645.\n",
      "Epoch 36/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.4668 - acc: 0.8297 - val_loss: 1.5262 - val_acc: 0.5288\n",
      "Epoch 37/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.4537 - acc: 0.8324 - val_loss: 1.5243 - val_acc: 0.5317\n",
      "Epoch 38/40\n",
      "23018/23018 [==============================] - 156s 7ms/step - loss: 0.4444 - acc: 0.8339 - val_loss: 1.5142 - val_acc: 0.5303\n",
      "Epoch 39/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.4393 - acc: 0.8403 - val_loss: 1.5723 - val_acc: 0.5282\n",
      "Epoch 40/40\n",
      "23018/23018 [==============================] - 155s 7ms/step - loss: 0.4327 - acc: 0.8382 - val_loss: 1.5593 - val_acc: 0.5282\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.007999999821186066.\n",
      "3197/3197 [==============================] - 5s 2ms/step\n",
      "[1.5636414607539937, 0.5283077887009188]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Activation, MaxPooling2D, BatchNormalization, GlobalMaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# num_classes = 10\n",
    "num_classes = 8\n",
    "num_epochs = 40\n",
    "num_batch = 32\n",
    "\n",
    "# # train data and test data for cifar10 dataset\n",
    "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# print(x_train.shape)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "# input : 64, 64 color RGB\n",
    "# conv1\n",
    "model.add(Conv2D(66, (3,3), padding='same', input_shape=x_train.shape[1:])) # change input shape\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Conv 2, 3 and 4\n",
    "model.add(Conv2D(128,(3,3),padding='same'))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "# Activation\n",
    "model.add(Conv2D(128,(3,3)))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "# Activation\n",
    "model.add(Conv2D(128,(3,3)))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Conv 5\n",
    "model.add(Conv2D(192, (3,3)))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# maxpooling2d\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Conv 6, 7, 8 and 9\n",
    "model.add(Conv2D(192,(3,3),padding='same'))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(192,(3,3),padding='same'))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(192,(3,3),padding='same'))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(192,(3,3),padding='same'))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Conv 9\n",
    "model.add(Conv2D(288, (3,3), padding='same'))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "# maxpooling2d\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "# Conv 10\n",
    "model.add(Conv2D(288, (3,3), padding='same'))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Conv 11\n",
    "model.add(Conv2D(355, (3,3)))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Conv 12\n",
    "model.add(Conv2D(432, (3,3)))\n",
    "# BatchNormalisation\n",
    "model.add(BatchNormalization(axis=-1, momentum=0.9, epsilon=0.001, scale=True))\n",
    "# Activation\n",
    "model.add(Activation('relu'))\n",
    "# DropOut \n",
    "model.add(Dropout(0.2))\n",
    "# global max pooling\n",
    "model.add(GlobalMaxPooling2D())\n",
    "\n",
    "# fully connected Layer MLP \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax', bias_initializer='constant'))\n",
    "\n",
    "\n",
    "# Initialise the optimizer Adam \n",
    "#optim = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "optim = Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "\n",
    "\n",
    "# Compile the model \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=optim, metrics = ['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# Normalize the data\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_train /= 255.0\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_test /= 255.0\n",
    "\n",
    "\n",
    "\n",
    "# If we are using data augmentation then we augment the images\n",
    "#horizontal/vertical flip and random rotation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "# apply the transformation on the train images\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# we want to reduce the LR once we hit a plateau \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Fit the model \n",
    "model.fit(x_train, y_train, batch_size=num_batch, epochs=num_epochs, callbacks=[reduce_lr], validation_split=0.2, shuffle=True )\n",
    "\n",
    "# Then we evaluate the model with evaluate()\n",
    "# and we print the model\n",
    "score = model.evaluate(x_test, y_test, batch_size=num_batch)\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MoodSimpNetV2-1DenseLayer-Softmax40.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nadir\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sadness\n",
      "Sadness\n",
      "Happiness\n",
      "Contempt\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Happiness\n",
      "Contempt\n",
      "Happiness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Contempt\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Contempt\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Happiness\n",
      "Sadness\n",
      "Happiness\n",
      "Happiness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Contempt\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Happiness\n",
      "Happiness\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Happiness\n",
      "Neutral\n",
      "Neutral\n",
      "Contempt\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Happiness\n",
      "Neutral\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Neutral\n",
      "Contempt\n",
      "Happiness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Contempt\n",
      "Contempt\n",
      "Contempt\n",
      "Contempt\n",
      "Happiness\n",
      "Happiness\n",
      "Contempt\n",
      "Neutral\n",
      "Neutral\n",
      "Happiness\n",
      "Happiness\n",
      "Contempt\n",
      "Happiness\n",
      "Contempt\n",
      "Contempt\n",
      "Contempt\n",
      "Contempt\n",
      "Contempt\n",
      "Contempt\n",
      "Contempt\n",
      "Contempt\n",
      "Happiness\n",
      "Neutral\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Happiness\n",
      "Neutral\n",
      "Anger\n",
      "Anger\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Sadness\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Happiness\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Neutral\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Neutral\n",
      "Anger\n",
      "Anger\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Anger\n",
      "Anger\n",
      "Neutral\n",
      "Anger\n",
      "Neutral\n",
      "Neutral\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Anger\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Anger\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Anger\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Sadness\n",
      "Sadness\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Anger\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Anger\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Anger\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Anger\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "nameList = []\n",
    "CATEGORIES = [\"Neutral\",\"Happiness\",\"Sadness\",\"Surprise\",\"Fear\",\"Disgust\",\"Anger\",\"Contempt\"]\n",
    "\n",
    "\n",
    "IMG_SIZE = 64\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('MoodSimpNetV2-1DenseLayer-Softmax40.h5')\n",
    "\n",
    "\n",
    "# Get a reference to webcam #0\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    #get a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    # resizing the frame for faster face recognition processing\n",
    "    faceFrame  = cv2.resize(frame, (0,0), fx=0.25, fy=0.25)\n",
    "    \n",
    "    # we take the face location(s) in the image \n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    # Notes: instead of saving the images we could \n",
    "    # pass the image straight to the CNN for classification\n",
    "    # but we will have to \n",
    "    for face_location in face_locations:\n",
    "        \n",
    "        \n",
    "        # carr√© 129 par 129\n",
    "        top, right, bottom, left = face_location\n",
    "\n",
    "#         print(frame)\n",
    "        # we create the image and scale it down to 64 x 64 \n",
    "        cropped_img = frame[left-100:right+10, top-50:bottom+50]\n",
    "        plt.imshow(cropped_img)\n",
    "        new_array = cv2.resize(cropped_img,(IMG_SIZE, IMG_SIZE))\n",
    "        new_array = cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB)\n",
    "        new_array = new_array[np.newaxis,:,:,:]\n",
    "        new_array = new_array.astype('float32')\n",
    "        new_array /= 255\n",
    "\n",
    "\n",
    "        \n",
    "        # we then classify the image and print the result\n",
    "        moodPrediction = model.predict(new_array)\n",
    "        print(CATEGORIES[np.argmax(moodPrediction)])\n",
    "        \n",
    "        # we draw rectangles around the faces\n",
    "#         cv2.rectangle(frame,(left-50,top-50), (right+50, bottom+50), (0,0,255), 2)\n",
    "        # we draw the labels with the names over, or below the face\n",
    "#         cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#         font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#         cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "        \n",
    "    # we Display the image\n",
    "    cv2.imshow('Video',frame)\n",
    "\n",
    "        # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('MoodSimpNetV2-1DenseLayer-Softmax40.h5')\n",
    "\n",
    "IMG_SIZE = 64\n",
    "\n",
    "img_array = cv2.imread('../Mood_Detector/Happiness/00fc0b2aab0d50be8cfe8793c65dfe019cf7ed6b04ec124d3f21f7b4.JPG')\n",
    "img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
    "new_array = cv2.resize(img_array,(IMG_SIZE, IMG_SIZE))\n",
    "new_array = new_array[np.newaxis,:,:,:]\n",
    "new_array = new_array.astype('float32')\n",
    "new_array /= 255\n",
    "\n",
    "\n",
    "new_array = model.predict(new_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.4451662e-08 9.9867320e-01 1.3431643e-08 2.3498749e-06 8.2302920e-09\n",
      "  1.8177511e-07 1.9980975e-08 1.3241724e-03]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
